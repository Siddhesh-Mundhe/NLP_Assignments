{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1cx-_aB20UC"
      },
      "outputs": [],
      "source": [
        "##perform  bag of words approach to count occurance normalised count occurance , TFIDF ON DATA. create embedding using word2vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample data (you can replace this with your own dataset)\n",
        "documents = [\"I love programming\", \"I hate bugs\", \"I love solving programming problems\"]\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the data to get the BoW representation\n",
        "X_bow = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the result to a DataFrame for better visualization\n",
        "import pandas as pd\n",
        "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(bow_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXcqGkFO32p2",
        "outputId": "b26b5a5e-f306-4be4-c774-707dd52ee03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   bugs  hate  love  problems  programming  solving\n",
            "0     0     0     1         0            1        0\n",
            "1     1     1     0         0            0        0\n",
            "2     0     0     1         1            1        1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "# Normalize the BoW representation using MaxAbsScaler\n",
        "scaler = MaxAbsScaler()\n",
        "normalized_bow = scaler.fit_transform(X_bow)\n",
        "\n",
        "# Convert normalized BoW to a dense array\n",
        "normalized_bow_array = normalized_bow.toarray()\n",
        "\n",
        "# Convert normalized BoW to a DataFrame for easier viewing\n",
        "normalized_bow_df = pd.DataFrame(normalized_bow_array, columns=vectorizer.get_feature_names_out())  # Use normalized_bow_array here\n",
        "print(normalized_bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70XvDbjV4KZu",
        "outputId": "617ebd37-bab3-4b0a-b6b2-0de95301f620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   bugs  hate  love  problems  programming  solving\n",
            "0   0.0   0.0   1.0       0.0          1.0      0.0\n",
            "1   1.0   1.0   0.0       0.0          0.0      0.0\n",
            "2   0.0   0.0   1.0       1.0          0.0      1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the data to get the TF-IDF representation\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the result to a DataFrame for better visualization\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0foLckBJ4OUe",
        "outputId": "8be379e3-a485-49bf-f0b7-e19a27172403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       bugs      hate      love  problems  programming   solving\n",
            "0  0.000000  0.000000  0.605349  0.000000     0.795961  0.000000\n",
            "1  0.707107  0.707107  0.000000  0.000000     0.000000  0.000000\n",
            "2  0.000000  0.000000  0.473630  0.622766     0.000000  0.622766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokenized_documents = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "# Save the model for later use\n",
        "model.save(\"word2vec.model\")\n",
        "\n",
        "# Access the vector for a specific word\n",
        "word_vector = model.wv['love']\n",
        "print(\"Word2Vec vector for 'love':\", word_vector)\n",
        "\n",
        "# Find most similar words to a given word\n",
        "similar_words = model.wv.most_similar('love', topn=5)\n",
        "print(\"Most similar words to 'love':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WegoGgyN4alu",
        "outputId": "d24b608e-7109-4d79-ce9d-d07256e317d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec vector for 'love': [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
            "Most similar words to 'love': [('solving', 0.06797593832015991), ('hate', 0.0093911774456501), ('bugs', 0.0045030200853943825), ('i', -0.010839177295565605), ('problems', -0.023671656847000122)]\n"
          ]
        }
      ]
    }
  ]
}